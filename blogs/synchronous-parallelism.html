<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Introduction to SIMD: Synchronous Parallelism</title>
    <meta name="description" content="How certain x86 instruction execute multiple operations in parallel.">

    <!-- OpenGraph tags-->
    <meta property="og:title" content="Introduction to SIMD: Synchronous Parallelism" />
    <meta property="og:image" content="https://tadejpetric.github.io/blogs/media/synchronous-parallelism-thumb.png" />
    <meta property="og:image:alt" content="x86-64 program that uses SIMD." />
    <meta property="og:image:width" content="690" />
    <meta property="og:image:height" content="314" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://tadejpetric.github.io/blogs/synchronous-parallelism.html" />
    <meta property="og:site_name" content="Tadej Petrič" />
    <meta property="og:description" content="How certain x86 instruction execute multiple operations in parallel." />

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Introduction to SIMD: Synchronous Parallelism" />
    <meta name="twitter:description" content="How certain x86 instruction execute multiple operations in parallel." />
    <meta name="twitter:image" content="https://tadejpetric.github.io/blogs/media/synchronous-parallelism-thumb.png" />
    <meta name="twitter:image:alt" content="x86-64 program that uses SIMD." />
    <meta name="twitter:site" content="@tadejpetric1" />
    <meta name="twitter:creator" content="@tadejpetric1" />



    <link rel="stylesheet" href="../assets/css/styles.css" />
    <link rel="stylesheet" href="../assets/css/blog_styles.css" />

    <!-- MathJax for LaTeX support -->
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

    <!-- highlight.js for syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/x86asm.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", (event) => {
            hljs.highlightAll();
        });
    </script>
</head>

<body>
    <header>
        <h1>Tadej Petrič</h1>
        <p>
            <a href="mailto:tadej.petric1@gmail.com">tadej.petric1@gmail.com</a> |
            <a href="https://www.linkedin.com/in/tadej-p-5b024987/">LinkedIn</a> |
            <a href="https://github.com/tadejpetric">GitHub</a>
        </p>
    </header>

    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../cv.html">CV</a></li>
            <li><a href="../blog.html">All Blog Posts</a></li>
            <li><a href="../cool-tidbits.html">Cool Tidbits</a></li>
        </ul>
    </nav>

    <main class="blog-content">
        <article>
            <h2>Introduction to SIMD: Synchronous Parallelism (Sequel to <a
                    href=https://tadejpetric.github.io/blogs/sequential-parallelism.html>Sequential Parallelism</a>)
            </h2>
            <p><em>Posted on March 24, 2025</em></p>

            <p>In the <a href=https://tadejpetric.github.io/blogs/sequential-parallelism.html>previous article</a> we
                looked at how processors can sometimes execute things in parallel, even though we never asked them to.
                We're continuing our journey of parallelism, except that this time, the parallelism is intended. The
                concept we'll explore this time is a bit more well-known (at least in my bubble), but not necessarily
                actually utilised. This concept is known as "Single instruction, multiple data" or SIMD for short.</p>
            <p>Let's first give a simple motivation, similar to the last article, where we were wondering why some
                processors have an extremely large amount of registers. This time, we take a look at the size of the
                registers. There are three common datatypes in computer languages that dominate the usage across the
                board: 32-bit integers, 32-bit floating points and 8-bit integer types (which includes characters and
                boolean numbers). If you add 64-bit integers (including pointers) and 64-bit floats, as well as composed
                structures using these types (classes), you cover well over 99% of the types in use. My processor, an
                i5-13400F, however, has some registers that are 256 bits large. Some desktop processors, such as the
                newer
                AMD CPUs and Intel's Xeon line, extend those to 512 bits. The most extreme
                architecture I know is the <a
                    href="https://www.nec.com/en/global/solutions/hpc/sx/architecture.html">NEC Vector Engine</a>, which
                has registers that are 16kb large! Why would that be useful if no datatypes in common use are actually
                that large?</p>
            <p>As in the last part of this series, we'll be using assembly to be as close to the hardware as possible.
                If you need a refresher, refer to the previous part of the series. I'll still explain any new
                instructions as they come up.</p>
            <h3>SIMD without extensions: Native vectorisation</h3>
            While x86 has many registers dedicated for SIMD, we will first look at how we can use the base operations
            within x86 to achieve improvements. Without any extensions at all. We will first consider an example of
            shifting values in an array and, after that, mention possible applications, including the problem of summing
            numbers from the previous post.

            <h4>Problem statement</h4>
            <p>Frequently in programming, we have to shift all values in an array. A simple example are insertions - if
                we want to insert an element at the index \(i\), we have to first shift all elements of the array from
                \(i\) onwards to create space for the newly inserted element. If insertions into an array were free,
                insertion sort would have the complexity \(O(n \log(n))\) rather than the \(O(n^2)\) it has in reality. If
                one wishes to implement a queue via a stack, they are also required to shift the array for every pop
                operation. Both algorithms are still frequently used today, especially for small input sizes, so let's
                see how we can optimize this operation.</p>

            <p>We are given an array of values and we wish to move every index at location \(i\) to location \(i+1\). We
                cycle the last element of the array to the front. For example, given an array</p>
            <pre>0, 1, 2, 3, 4, 5</pre>
            <p>The output after one shift will be</p>
            <pre>5, 0, 1, 2, 3, 4.</pre>
            <p>Crucially, each value in the array is one byte in size and the size of the array is chosen to avoid
                out-of-bounds errors - it's divisible by a sufficiently large power of two.</p>
            <details>
                <summary>Click to see the benchmarking code.</summary>
                <p>We define an array <code class="language-x86asm">array</code> with two billion elements, each of size
                    one byte. We initialise it with some garbage values. We then call the function <code
                        class="language-x86asm">shift_array</code> ten times.</p>
                <pre><code class="language-x86asm">; code that shifts an array 1 byte to the right
; This is cyclical, ie. the last byte becomes the first
GLOBAL main
%define array_len 500_000_000

SECTION .bss
    ; We use 32-bit integers here for ease of initialisation.
    ; We are still shifting the array as if it were bytes.
    array resd array_len ; We are reserving 500M * 32 bits

SECTION .text

set_array:
    xor eax, eax ; array index

    startsetloop:
        cmp eax, array_len ; if eax >= array_len, go to endsetloop
        jge endsetloop
        mov [array + 4*eax], eax ; array[eax] = eax
        inc eax ; eax++
        jmp startsetloop
    endsetloop:

    ret

shift_array:
    ; Procedure that shifts the array
    ret

main:
    call set_array
    xor eax, eax ; eax = 0

    startmainloop: ; call shift_array 10 times
        cmp rax, 10
        jge endmainloop
        push rax
        call shift_array
        pop rax
        inc rax
        jmp startmainloop
    endmainloop:
    
    xor eax, eax
    ret

</code></pre>
            </details>
            <p>The empty runtime on my computer (an i5-13400F), without any shifts (just starting up the program and
                initialising the array) is 330ms. Anything over that is the runtime taken by our shift code.</p>
            <h4>Naive approach</h4>
            <p>We first try to solve the problem naively. We have two temporary registers, <code
                    class="language-x86asm">al</code> and <code class="language-x86asm">bl</code>. For each index, we
                store the current byte in <code class="language-x86asm">bl</code>, write the value from <code
                    class="language-x86asm">al</code> into the current position (i.e., the previous value), then update
                <code class="language-x86asm">al</code> with <code class="language-x86asm">bl</code> to carry it into
                the next iteration. We are left with the
                following code:
            </p>
            <pre><code class="language-x86asm">shift_array:
    xor eax, eax ; previous value carrier
    xor ecx, ecx ; array index

    startshiftloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endshiftloop
        jge endshiftloop
        mov bl, [array + rcx]  ; \
        mov [array + rcx], al  ;  - swap(al, array[rcx])
        mov al, bl             ; /
        inc rcx ; rcx++
        jmp startshiftloop
    endshiftloop:

    mov [array], al ; Cycle the last element to the front
    ret</code></pre>
            <p>This naive implementation takes 1.595s on average, giving us a performance baseline to beat.</p>
            <details>
                <summary>Tangent 1: Swapping natively is slow.</summary>
                <p>In x86 there is actually an instruction that swaps two elements. This instruction allows swapping
                    without needing an intermediate register. The code above could be rewritten as:</p>
                <pre><code class="language-x86asm">shift_array:
    xor eax, eax ; previous value carrier
    xor ecx, ecx ; array index

    startshiftloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endshiftloop
        jge endshiftloop
        xchg al, [array + rcx] ; swap(al, array[rcx]) 
        inc rcx ; rcx++
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret</code></pre>
                <p>Surprisingly, this version ends up being over ten times <em>slower</em> than our naive approach -
                    taking a whopping 19.136s! This slowdown is primarily due to <code
                        class="language-x86asm">xchg</code> being an atomic operation (which we might look at in the
                    last chapter of the blog series) and likely also affected by instruction-level dependency (as
                    discussed in the previous post).</p>
            </details>
            <details>
                <summary>Tangent 2: A clever approach.</summary>
                <p>There is another approach to the program. This time, we iterate <em>backwards</em> through the array.
                    We only have to move each element one byte forward, directly overwriting any elements at that
                    position. This way we don't need a temporary variable for swapping. However, we still need a
                    temporary register since x86 doesn't support memory-to-memory moves - one operand must be a register.
                </p>
                <pre><code class="language-x86asm">shift_array:
    mov rcx, array_len ; array index
    dec rcx
    mov al, [array + rcx] ; Store the last value for the cyclical shift

    startshiftloop:
        cmp rcx, 0 ; if rcx <= 0, go to endshiftloop
        je endshiftloop
        ; mov byte [array + rcx], byte [array + rcx - 1] 
        ; we can't do direct memory to memory, so intermediate bl is needed
        mov bl, byte [array + rcx - 1]
        mov byte [array + rcx], bl
        dec rcx ; rcx--
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret</code></pre>
                <p>This code turns out to be marginally faster, averaging 1.435s.</p>
            </details>
            <h4>Vectorisation</h4>
            <p>Now we've arrived at the crux of the post: vectorisation. Rather than just moving one element at a time,
                we can move several elements at once. Until now, we've been accessing just one value at a time. But our
                processor has 64-bit general-purpose registers, meaning we can read and write 8 elements at once! To
                move the bytes forward, we simply bit shift them left for 8 bits - since shifting removes the highest
                byte, so we store it temporarily before the shift and restore it in the next iteration. Since we only
                have to do one memory read / write per 8 bytes, the resulting code ends up much faster.</p>
            <pre><code class="language-x86asm">shift_array:
    xor eax, eax ; previous value carry
    xor ecx, ecx ; array index

    startshiftloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endshiftloop
        jge endshiftloop
        mov rbx, [array + rcx] ; we read 8 bytes (rbx instead of bl)
        mov rdx, rbx ; a copy for extracting the high byte

        shl rbx, 8 ; shift the values one byte
        or rbx, rax ; add left most byte from the previous iteration

        shr rdx, 56  ; get the left most byte
        mov rax, rdx ; store them into rax for the next iteration

        mov [array + rcx], rbx

        add rcx, 8 ; rcx += 8, we are moving 8 bytes forward
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret </code></pre>
            <p>This code has an average runtime of 0.701s. Accounting for runtime of the initialisation, this is roughly
                a 4x speedup! We achieved this speedup by using a single instruction (64-bit read and shift) to operate
                on multiple pieces of data (8 bytes — each one an array element). That's SIMD in action.</p>

            <details>
                <summary>Tangent 3: A clever approach, part 2.</summary>
                <pre><code class="language-x86asm">shift_array:
    mov rcx, array_len ; array index
    dec rcx
    mov al, [array + rcx] ; Store the last value for the cyclical shift
    sub rcx, 7

    startshiftloop:
        cmp rcx, 0 ; if rcx <= 0, go to endshiftloop
        je endshiftloop
        mov rbx, [array + rcx - 1]
        mov [array + rcx], rbx ; move the 8 bytes one byte forward
        sub rcx, 8 ; rcx -= 8
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret</code></pre>
                <p>If we attempt to use the same clever approach with the SIMD method, our program runtime ends up at
                    0.675s. This slight improvement likely comes from reduced overhead due to fewer bitwise operations.
                </p>
            </details>

            <h4>Other examples</h4>
            <p>Native vectorisation techniques like these can be applied in other scenarios too. For example, if we want
                to increment each byte of the array we can either iterate byte by byte and increment each one or we
                could process 8 bytes at once and add <code class="language-x86asm">0x01010101_01010101</code> to the
                entire pack of 8. Naturally, this only works if there is no risk of overflows, but since signed overflow
                is undefined behaviour in many languages (including C), this isn't as unreasonable as it might seem. The
                overflow restriction, however, stops us from solving the previous blog's topic, the sum of an array,
                with this approach (at least in an efficient way). We could still attempt to reduce the number of memory
                accesses by reading 64 bits from memory, rather than 32 bits, but general-purpose registers only support
                efficient 32-bit additions at a time. As a result, even reading 64 bits at once doesn't meaningfully
                improve performance - my benchmarks showed only about a 1% speedup.</p>
            <p>Another trivial example are the binary operations - they're already inherently vectorised. For instance,
                the <code class="language-x86asm">and</code> instruction applies the bitwise AND to every bit in the
                register simultaneously.</p>
            <p>An interesting example can be found using the multiplication operation. If we multiply a byte with the
                number <code class="language-x86asm">0x0101</code>, we duplicate that byte. For example
                <code>0x42 * 0x0101 = 0x4242</code>. We can increase that even further, for example the multiplication
                by <code class="language-x86asm">0x01010101</code> would quadruple the byte, etc.
            </p>
            <p>There are countless other low-level tricks like this (for more, I highly recommend the book Hacker's
                Delight). But for now, let's move on to the part of the CPU architecture purpose-built for SIMD.</p>
            <h3>MMX</h3>
            <p>Eventually, the CPU designers realised such operations are useful and extended the x86 architecture with
                new registers and operations designed for SIMD. The first such extension was <a
                    href="https://en.wikipedia.org/wiki/MMX_(instruction_set)">MMX</a>. These days it's hard to find a
                processor without MMX support as all mainstream x86-64 Intel and AMD processors support it. MMX comes
                with additional 64-bit registers. The catch, however, is that instructions operate on several smaller
                values packed inside the register, rather than treating it as a single 64-bit value. Performing addition
                on such a register doesn't add two 64-bit numbers together, instead it might compute the sum of the
                first 32 bits and the last 32 bits separately.</p>
            <p>Instructions offered in the MMX set are pretty basic. MMX provides 8 dedicated 64-bit registers. There
                are instructions operating on these registers, as for the standard x86 set. The instructions operate on
                multiple data stored inside the register; when we do addition of two MMX registers, it adds the
                constituents independently; similar to performing two 32-bit additions, four 16-bit additions, or eight
                8-bit additions - all in parallel. Multiple values are "packed" inside the register. The base MMX set
                includes instructions to:
            <ul>
                <li>Move data in and out of registers</li>
                <li>Pack data together (e.g., combining two registers holding 32-bit integers into one register with
                    four 16-bit integers)</li>
                <li>Apply arithmetic operations with overflow</li>
                <li>Apply arithmetic operations with saturation (i.e., clamp values to max/min on overflow)</li>
                <li>Apply logical operations</li>
                <li>Apply bitwise operations and bit shifts</li>
            </ul>
            </p>

            <p>None of these MMX instructions are particularly useful for the array-shift problem, so let's return to
                the array summation problem from the previous post. Our goal is to compute the sum of the array and
                store it in a 32-bit general-purpose register. We are only interested in the sum modulo \(2^32\), so overflow doesn't
                matter.</p>

            <details>
                <summary>Benchmark code</summary>
                <p>The code is virtually identical to the one for bit shifting, as well as identical to the one from the
                    previous blog post. I am sharing it regardless for completeness.</p>
                <pre><code class="language-x86asm">GLOBAL main
%define array_len 500_000_000

SECTION .bss
    array resd array_len

SECTION .text

set_array:
    xor eax, eax ; array index

    startsetloop:
        cmp eax, array_len ; if eax >= array_len, go to endsetloop
        jge endsetloop
        mov [array + 4*eax], eax ; array[eax] = eax
        inc eax ; eax++
        jmp startsetloop
    endsetloop:

    ret

sum_array:
    ; Procedure that sums the array
    ret

main:
    call set_array
    xor eax, eax ; eax = 0

    startmainloop: ; call sum_array 10 times
        cmp rax, 10
        jge endmainloop
        push rax
        call sum_array
        pop rax
        inc rax
        jmp startmainloop
    endmainloop:
    
    xor eax, eax
    ret</code></pre>
            </details>

            <p>The initialisation of the array takes roughly 300ms as before. The naive approach takes 1.669s on average
            </p>
            <pre><code class="language-x86asm">sum_array:
    xor eax, eax ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        add eax, [array + 4*rcx] ; eax += array[rcx]
        inc rcx ; rcx++
        jmp startsumloop
    endsumloop:

    ret
</code></pre>

            <p>This time, we'll solve it using MMX registers. We'll start with a toy program to introduce MMX registers
                and their operations, then quickly write the code for summing the array.</p>

            <p>First, we have to mention that the MMX registers are actually shared with the floating point registers.
                This doesn't affect us here, since we aren't using any floating-point instructions, but do not mix MMX
                code and
                x87 floating-point code. That's also the reason we have to use the instruction <code
                    class="language-x86asm">emms</code> after finishing with MMX instructions, it exits the MMX mode and
                resets the FPU state. We address the registers using <code class="language-x86asm">mm0, ..., mm7</code>.
                Unlike x87 FPU registers, MMX registers are directly addressable - no stack required. They
                also have their own operations. For instance, instead of <code class="language-x86asm">mov</code> we
                have to use <code class="language-x86asm">movq</code>, instead of <code
                    class="language-x86asm">add</code> we use <code class="language-x86asm">paddd</code> and so on.
                Let's look at a code sample.</p>
            <pre><code class="language-x86asm">mov eax, 1
mov ebx, 6
movd mm0, eax ; mm0 has 0x1
movd mm1, ebx ; mm1 has 0x6

xor eax, eax ; clear data. Not needed
xor ebx, ebx ; Just to demonstrate we are using mmx only

packssdw mm0, mm1 ; (1) mm0 has 0x0000000100000006

; value dq 0x0000000400000002
movq mm1, [value] ; mm1 has 0x0000000400000002

paddd mm0, mm1 ; (2) mm0 has 0x0000000500000008

movd eax, mm0 ; eax has 0x8
psrlq mm0, 32 ; (3) bitshift the high 32 bits into the low 32 bits
movd ebx, mm0 ; ebx has 0x5

emms

add eax, ebx ; eax has 0xC = 0x8 + 0x5
</code></pre>

            <p>Some parts of the code are pretty self-explanatory. We initialise <code
                    class="language-x86asm">[value]</code> with a 64-bit value which we interpret as two 32-bit integers
                (sometimes called doublewords). We can
                move data into the lowest part of a MMX register using <code class="language-x86asm">movd</code> (d
                stands for doubleword, 32 bits; q would stand for quadword or 64 bits). At step (1) we pack two MMX
                registers into one. The mnemonic <code class="language-x86asm">packssdw</code> stands for <strong>pack
                </strong>, using <strong>S</strong>igned <strong>S</strong>aturation, 4 <strong>D</strong>oublewords
                (two from MM0,
                two from MM1) into 4 <strong>W</strong>ords. So it casts both 32-bit components of MMX registers into
                two 16-bit
                components (and, if they are too large to fit, saturates them by setting them to the highest 16-bit
                value), then concatenates them to the (also converted and saturated) 16-bit components of the other MMX
                register.</p>

            <p>With <code class="language-x86asm">paddd</code> at (2) we add both 32-bit components independently.
                We compute the sum of the first 32 bits of both registers and store it into the first 32 bits of mm0;
                then we compute the sum of the last 32 bits of both registers and store it into the second half of mm0.
                If we wanted to add 8 separate byte-large components, we would use <code
                    class="language-x86asm">paddb</code> - useful for arrays of 8-bit values. Lastly, we apply bitshift
                to the entire quadword MMX register,
                to move the highest 32 bits (representing the other integer) into the lower 32 bits, which can
                then be easily extracted into the general-purpose registers. The final result is 13, as desired.</p>

            <p>We now write code that uses MMX to compute the sum of an array.</p>
            <pre><code class="language-x86asm">sum_array:
    pxor mm0, mm0 ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        paddd mm0, [array + 4*rcx] ; Compute sum of two elements of the array
        add rcx, 2 ; rcx+=2, we are processing two elements at a time
        jmp startsumloop
    endsumloop:

    movd eax, mm0
    psrlq mm0, 32
    movd ecx, mm0
    add eax, ecx

    emms

    ret</code></pre>
            <p>Compared to the naive version, this program computes two partial sums in parallel using 64-bit MMX
                registers. The average runtime now is
                approximately 1.330s. Interestingly - and somewhat expectedly - this is roughly as fast as our final
                program in the previous post, where we computed two sums in parallel, just like here. However that
                version relied on
                instruction-level parallelism, while this one uses SIMD. Now in the words of Goku: "And this is to go
                even further beyond!"; we explore other extensions of x86 which offer even higher levels of SIMD
                parallelism, as well as more flexibility. MMX itself is quite dated and some of the design choices are
                questionable (for example aliasing the registers to the FPU).</p>

            <h3>SSE</h3>
            <p><a href="https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions">SSE</a> stands for Streaming SIMD
                Extensions. It was originally introduced in 1999, but it went through a few revisions and we're
                currently
                at SSE4.2. This extension adds 8 new XMM registers (16 in x86-64. Do not confuse XMM with MMX).
                Crucially, these registers are no longer 64-bit, they are 128 bits wide and later extended by the
                AVX extension, which we will cover at the end. The original SSE allowed SIMD operations of four
                32-bit floats. SSE2 extended the support to 2 64-bit floats, as well as all kinds of integer SIMD
                instructions (for example operations on four 32-bit integers packed into a 128-bit register). SSE3 added
                some instructions that are not just the default SIMD, but include horizontal operations, which are
                operations within the same register rather than across registers. For example,
                <code class="language-x86asm">haddpd</code> takes two 128-bit XMM registers, the first one storing
                64-bit floats \(a_1\) and \(a_2\) and the second one storing 64-bit floats \(b_1\) and \(b_2\). The
                result of the operation is an XMM register storing two 64-bit floats: \(a_1+a_2\) and \(b_1+b_2\)
                (whereas vertical addition would carry \(a_1+b_1\) and \(a_2+b_2\)). SSE4 includes a variety of new
                instructions. I will not touch upon all of them as they can be a bit niche, but the most useful
                (in my opinion) are <code class="language-x86asm">DPPS</code> (dot product of packed single-precision)
                for computing a dot product and <code class="language-x86asm">pminsd</code> (packed minimum of signed
                doublewords), which computes a packed minimum. You can check the full
                instruction set here: <a
                    href="https://www.intel.com/content/dam/develop/external/us/en/documents/d9156103-705230.pdf">https://www.intel.com/content/dam/develop/external/us/en/documents/d9156103-705230.pdf</a>.
            </p>

            <p>Now we'll write a four-way parallel version of the sum program, this time using the 128-bit SSE
                instructions, rather than the 64-bit MMX ones. We are using three operations. One, <code
                    class="language-x86asm">paddd</code> is simple, it just adds four 32-bit integers at once. The
                instruction <code class="language-x86asm">movhlps</code> is a bit more complex. It takes a register
                containing four 32-bit values</p>
            <pre>xmm0 = [a, b, c, d]</pre>
            <p> and moves the values from the high 64 bits to the low 64 bits. The result of the operation <code
                    class="language-x86asm">movhlps xmm1, xmm0</code> is </p>
            <pre>xmm1 = [c, d, ?, ?]</pre>
            <p>Lastly, we need <code class="language-x86asm">pshufd</code>. This allows us to shuffle the integers in an
                XMM register arbitrarily. It takes three operands, the destination, the source register, and an 8-bit
                shuffle control value. As the shuffle control value is an 8-bit integer, it can be
                partitioned into four 2-bit chunks. Each chunk tells you which integer from the source register it gets.
                Let's consider an example:</p>
            <pre>xmm0 = [a, b, c, d]</pre>
            <p>With the control value 100, which is 0b01 10 01 00 in binary. We convert each chunk back to decimal,
                to get 1, 2, 1, and 0. Thus the destination gets the 2nd element of source in the first element of the
                destination (2nd element due to zero indexing), 3rd element of source in the second element of the
                destination and so on. So the output of <code class="language-x86asm">pshufd xmm1, xmm0, 100</code> is
            </p>
            <pre>xmm1 = [b, c, b, a]</pre>
            <p>One last warning before we write the code: the data we're operating on has to be 16-byte aligned. In
                our benchmark code, this is achieved using <code class="language-x86asm">align 16</code> before the
                declaration of the array. Now it's time to write our code.</p>

            <pre><code class="language-x86asm">sum_array:
    pxor xmm0, xmm0 ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        paddd xmm0, [array + 4*rcx]
        add rcx, 4 ; rcx+=4, we are processing 4 elements per iteration
        jmp startsumloop
    endsumloop:

    ; xmm0 has 32-bit integers [a, b, c, d] 
    movhlps xmm1, xmm0 ; Move high bits from xmm0 -> low bits of xmm1
    ; xmm1 has [c, d, ?, ?]
    paddd xmm0, xmm1
    ; xmm0 has [a+c, b+d, ?, ?]
    pshufd xmm1, xmm0, 0x1
    ; xmm1 has [b+d, a+c, a+c, a+c]
    paddd xmm0, xmm1
    ; xmm0 has [a+c + b+d, b+d + a+c, ?, ?]
    movd eax, xmm0
    ; eax has a+c + b+d = a+b+c+d = sum of the array

    ret
</code></pre>
            <p>As expected, due to the increased parallelism, this version is even faster and the runtime is now
                approximately
                1.181s. Let's move on to the next SIMD extension.</p>

            <h3>AVX</h3>
            <p><a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a> stands for Advanced Vector
                Extensions. This one, too, comes in several versions: AVX, AVX2 and AVX-512 (and technically AVX10, but
                this
                one doesn't really add anything). The extension further extends the SSE registers to 256 bits or, in the
                case of AVX-512, to 512 bits (or 64 bytes, capable of storing 16 32-bit integers). It also comes with a
                more
                complex coding scheme, <a href="https://en.wikipedia.org/wiki/VEX_prefix">VEX coding</a>. Previously,
                all
                instructions only worked on up to two operands, and optionally a fixed constant (an immediate),
                as we saw in <code class="language-x86asm">pshufd</code>. The new scheme allows up to four
                operands (and an additional immediate). Most instructions are direct extensions of their SSE
                counterparts, adapted to work with the larger registers and VEX encoding, but AVX also adds some new
                instructions, mostly for shuffling and rearranging the data in registers.</p>
            <p>If SSE registers are addressed using <code class="language-x86asm">xmm0, ..., xmm15</code>, the 256-bit
                AVX registers are addressed using <code class="language-x86asm">ymm0, ..., ymm15</code> and the 512-bit
                ones
                using <code class="language-x86asm">zmm0, ..., zmm15</code>. If our CPU supports AVX-512, we have 32
                registers, so the registers range to <code class="language-x86asm">xmm31 / ymm31 / zmm31</code>. While
                every
                x86
                computer supports MMX and every modern computer supports SSE, AVX support is still a bit rare and
                AVX-512
                even more so. I actually don't have access to AVX-512 hardware at home, so I rely on a
                university-provided HPC cluster for testing.</p>


            <details>
                <summary>Tangent 4: VEX coding on non-vector registers.</summary>
                <p>Although this wasn't originally the case, VEX coding has since been extended to cover general-purpose
                    registers, such as <code class="language-x86asm">eax</code>. However, their use is mostly limited to
                    the bit
                    manipulation instructions. Let's check one example.</p>
                <pre><code class="language-x86asm">mov eax, 0x0000_0F0F
mov ebx, 0x0000_F00F
mov ecx, ebx ; copy ebx
not ecx ; ~ebx = 0xFFFF_0FF0
and ecx, eax ; ecx = ~ebx & eax = 0x0000_0F00 = 3840
            </code></pre>
                <p>With VEX coding, we can rewrite this as:</p>
                <pre><code class="language-x86asm">mov eax, 0x0000_0F0F
mov ebx, 0x0000_F00F
andn ecx, ebx, eax
</code></pre>
            </details>

            <details>
                <summary>Tangent 5: false AVX dependencies.</summary>
                <p>AVX extensions made an interesting choice with their use of the SSE registers. See, when we use
                    32-bit instructions on 64-bit general purpose registers, the upper 32 bits are implicitly zeroed out.
                    That's why we typically zero a register using <code class="language-x86asm">xor eax, eax</code>
                    instead of <code class="language-x86asm">xor rax, rax</code>. It's slightly more efficient (it's one
                    byte shorter in machine code) and the output is the same in both cases, every bit of the 64-bit register <code
                        class="language-x86asm">rax</code> becomes zero. However when we use the SSE instructions
                    (without their VEX-coded form) <em>that doesn't happen</em>. Only the lower 128 bits are modified.
                    This can lead to false dependencies, making it harder for the CPU to optimize the pipeline (exactly the 
                    kind of optimisation we relied on in the previous blog post, in
                    fact). They added an instruction <code class="language-x86asm">vzeroupper</code> for this (which zeroes out
                    the upper bits, eliminating the stalls), but this still adds overhead. We can bypass that by not mixing
                    SSE and AVX or, if we do have to mix them, use the VEX coded form of SSE operations, which does zero out
                    the higher bits.</p>
                <p>A similar thing sometimes happens with the use of <code class="language-x86asm">inc</code>
                    (increment)
                    instruction. While the <code class="language-x86asm">add eax, 1</code> does the full update of the
                    condition flags, <code class="language-x86asm">inc eax</code> only does a partial update which can
                    cause some subtle stalls due to false dependencies and slow the code down. This behavior was largely mitigated after the Pentium 4 era, and on modern CPUs, there's usually no measurable difference between the two.</p>
            </details>

            <p>Let's move on to the code. This time we extract the data from the AVX registers a bit differently than before.
                A similar method to before would still work, but as I am using this blog post to give an introduction to
                the new SIMD instructions, I believe it would be beneficial to introduce some more instructions. The first is
                <code class="language-x86asm">vextractf128</code>. This one can extract 128 bits from a 256-bit register
                and store it into a 128-bit register. The second instruction is <code class="language-x86asm">phaddd</code>,
                which performs horizontal addition. So given two registers <code
                    class="language-x86asm">xmm0 ; = [a, b, c, d]</code> and <code
                    class="language-x86asm">xmm1 ; = [e, f, g, h]</code>, the result of <code
                    class="language-x86asm">phaddd xmm0, xmm1</code> is <code
                    class="language-x86asm">xmm0 ; = [a+b, c+d, e+f, g+h]</code>. We can use this to coalesce the
                results from 8 independent integers into one sum.
            </p>
            <p>We also need to ensure that our memory is aligned to 32-byte boundaries.</p>
            <pre><code class="language-x86asm">SECTION .bss
    align 32 ; 64 for AVX-512
    array resd array_len</code></pre>
            <p>We now arrive at the AVX-based version of the sum:</p>
            <pre><code class="language-x86asm">sum_array:
    vpxor ymm0, ymm0, ymm0 ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        vpaddd ymm0, ymm0, [array + 4*rcx] ; add 8 32-bit integers from memory into ymm0
        add rcx, 8 ; rcx+=8, we are summing 8 elements at once
        jmp startsumloop
    endsumloop:

    ; ymm0 = [a0, a1, a2, a3, a4, a5, a6, a7]
    vextractf128 xmm1, ymm0, 1 ; xmm1 = [a4, a5, a6, a7]
    paddd xmm0, xmm1 ; xmm0 = [a0+a4, a1+a5, a2+a6, a3+a7]
    phaddd xmm0, xmm0 ; xmm0 = [ (a0+a4)+(a1+a5), (a2+a6)+(a3+a7) ]
    phaddd xmm0, xmm0 ; xmm0[0] = (a0+a4)+(a1+a5)+(a2+a6)+(a3+a7)

    movd eax, xmm0
    ret</code></pre>
            <p>And the speed of the code is now 1.074s. At this point, we begin to see diminishing returns, but there's still 
                a measurable speedup. Lastly, the code using AVX-512. This is included more for completeness. I won't dwell on
                 it too much due to limited availability of AVX-512 hardware. The code is largely the
                same as the 256-bit version.</p>
            <pre><code class="language-x86asm">sum_array:
    vpxord zmm0, zmm0, zmm0 ; partial sum carrier
    xor ecx, ecx ; array index
    
    startsumloop:
        cmp rcx, array_len ; if index >= array_len, go to endsumloop
        jge endsumloop
        vpaddd zmm0, zmm0, [array + 4*rcx]
        add rcx, 16 ; rcx+=16, we are summing 16 elements at once
        jmp startsumloop
    
    endsumloop:
        ; zmm0 contains 16 partial sums.
        vextracti32x8 ymm1, zmm0, 1 ; Extract upper 256 bits into ymm1
        vpaddd ymm0, ymm0, ymm1 ; Add lower and upper 256-bit halves => ymm0 now has 8 ints
        vextracti32x4 xmm1, ymm0, 1 ; Extract upper 128 bits into xmm1
        vpaddd xmm0, xmm0, xmm1 ; Add lower and upper 128-bit halves => xmm0 now has 4 ints
        phaddd xmm0, xmm0, xmm0 ; Horizontal add: now xmm0 contains 2 partial sums in its lower half
        phaddd xmm0, xmm0, xmm0 ; Final horizontal add: xmm0[0] holds the total sum
    
        movd eax, xmm0
        ret
    </code></pre>
            <h3>Summary of results</h3>
            <p>Now let's bring all results together. I am also running the code on another computer, an HPC with a Xeon
                Silver 4410Y. This way, we can also measure the impact of AVX-512, but the performance will not be good
                as it's currently also running other things (it is, however, consistent).</p>
            <table>
                <tr>
                    <th></th>
                    <th>i5-13400F</th>
                    <th>Xeon 4410Y</th>
                </tr>
                <tr>
                    <td>Naive</td>
                    <td>1.669s</td>
                    <td>3.400s</td>
                </tr>
                <tr>
                    <td>MMX</td>
                    <td>1.330s</td>
                    <td>3.076s</td>
                </tr>
                <tr>
                    <td>SSE</td>
                    <td>1.181s</td>
                    <td>2.717s</td>
                </tr>
                <tr>
                    <td>AVX</td>
                    <td>1.074s</td>
                    <td>2.364s</td>
                </tr>
                <tr>
                    <td>AVX-512</td>
                    <td>N/A</td>
                    <td>2.031s</td>
                </tr>
            </table>

            <p>The results we obtained are quite satisfying. We managed to obtain a massive speedup by using the
                SIMD instructions and, hopefully, learned something along the way. As a curiosity, if we compile the
                naively written C program with GCC using the highest optimisation levels and <code>-march=native</code>, it
                compiles it to a version roughly equivalent to our AVX code, both in structure and performance. Can you combine the
                approaches of this post and the last one, using both SIMD and instruction-level parallelism to beat GCC?</p>
            <p>Regardless, it's still valuable to understand SIMD instructions, as sometimes the compiler won't recognise the optimisations (or only
                recognises them if the code is written in a very specific way). There's another interesting SIMD extension,
                AMX (Advanced Matrix Extensions), but given that it's limited to matrices and matrix multiplication, I feel
                like it's beyond the scope of this article.</p>

            <p>If you have any further questions or comments, drop me an email! I would be happy to discuss anything
                further.</p>
        </article>
    </main>
    <!-- mention GPUs -->
    <footer>
        <p>© 2025 Tadej Petrič</p>
    </footer>
</body>

</html>
