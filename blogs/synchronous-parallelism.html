<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Introduction to SIMD: Sequential Parallelism</title>
    <meta name="description" content="How certain x86 instruction execute multiple operations in parallel.">

    <!-- OpenGraph tags-->
    <meta property="og:title" content="Introduction to SIMD: Sequential Parallelism" />
    <meta property="og:image" content="https://tadejpetric.github.io/blogs/media/synchronous-parallelism-thumb.png" />
    <meta property="og:image:alt" content="x86-64 program that uses SIMD." />
    <meta property="og:image:width" content="690" />
    <meta property="og:image:height" content="314" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://tadejpetric.github.io/blogs/synchronous-parallelism.html" />
    <meta property="og:site_name" content="Tadej Petrič" />
    <meta property="og:description" content="How certain x86 instruction execute multiple operations in parallel." />

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Introduction to SIMD: Sequential Parallelism" />
    <meta name="twitter:description" content="How certain x86 instruction execute multiple operations in parallel." />
    <meta name="twitter:image" content="https://tadejpetric.github.io/blogs/media/sequential-parallelism-thumb.png" />
    <meta name="twitter:image:alt" content="x86-64 program that uses SIMD." />
    <meta name="twitter:site" content="@tadejpetric1" />
    <meta name="twitter:creator" content="@tadejpetric1" />



    <link rel="stylesheet" href="../assets/css/styles.css" />
    <link rel="stylesheet" href="../assets/css/blog_styles.css" />

    <!-- MathJax for LaTeX support -->
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

    <!-- highlight.js for syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/x86asm.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", (event) => {
            hljs.highlightAll();
        });
    </script>
</head>

<body>
    <header>
        <h1>Tadej Petrič</h1>
        <p>
            <a href="mailto:tadej.petric1@gmail.com">tadej.petric1@gmail.com</a> |
            <a href="https://www.linkedin.com/in/tadej-p-5b024987/">LinkedIn</a> |
            <a href="https://github.com/tadejpetric">GitHub</a>
        </p>
    </header>

    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../cv.html">CV</a></li>
            <li><a href="../blog.html">All Blog Posts</a></li>
            <li><a href="../cool-tidbits.html">Cool Tidbits</a></li>
        </ul>
    </nav>

    <main class="blog-content">
        <article>
            <h2>Introduction to SIMD: Synchronous Parallelism (Sequel to <a
                    href=https://tadejpetric.github.io/blogs/sequential-parallelism.html>Sequential Parallelism</a>)
            </h2>
            <p><em>Posted on March 24, 2025</em></p>

            <p>In the <a href=https://tadejpetric.github.io/blogs/sequential-parallelism.html>previous article</a> we
                looked at how processors can sometimes execute things in parallel, even though we never asked them to.
                We're continuing our journey of parallelism, except that this time, the parallelism is intended. The
                concept we'll explore this time is a bit more well known (at least in my bubble), but not necessarily
                actually utilised. This concept is known as "Single instruction, multiple data" or SIMD for short.</p>
            <p>Let's first give a simple motivation, similar to the last article, where we were wondering why some
                processors have an extremely large amount of registers. This time, we take a look at the size of the
                registers. There are three common datatypes in computer languages that dominate the usage across the
                board: 32-bit integers, 32-bit floating points and 8 bit integer types (which includes characters and
                boolean numbers). If you add 64-bit integers (including pointers) and 64-bit floats, as well as composed
                structures using these types (classes), you cover well over 99% of the types in use. My processor, an
                i5-13400F, however, has some registers that are 256 bits large. Some desktop processors, like the new
                AMD processors and Intel's Xeon line of processors extend those to 512 bits. The most extreme
                architecture I know is the <a
                    href="https://www.nec.com/en/global/solutions/hpc/sx/architecture.html">NEC Vector Engine</a>, which
                has registers that are 16kb large! Why would that be useful if no datatypes in common use are actually
                that large?</p>
            <p>As in the last part of this series, we'll be using assembly to be as close to the hardware as possible.
                If you need a refresher, look at the last part of the series. I'll be explaining any new instructions,
                however.</p>
            <h3>Native vectorisation</h3>
            While x86 has many registers dedicated for SIMD, we will first look at how we can use the base operations
            within x86 to achieve improvements. Without any extensions at all. We will first consider an example of
            shifting values in an array and after that mention possible applications, including the problem of summing
            numbers from the previous post.

            <h4>Problem statement</h4>
            <p>Frequently in programming, we have to shift all values in an array. A simple example are insertions - if
                we want to insert an element at the index \(i\), we have to first shift all elements of the array from
                \(i\) onwards to create space for the newly inserted element. If insertions in an array were free,
                insertion sort would have the complexity \(O(n \log(n))\) rather than the \(O(n)\) it has in reality. If
                one wishes to implement a queue via a stack, are also required to shift the array for every pop
                operation. Both of the algorithms are frequently used today (for small input sizes), so let's try to
                optimise it.</p>

            <p>We are given an array of values and we wish to move every index at location \(i\) to location \(i+1\). We
                cycle the last element of the array to the front. For example, given an array</p>
            <pre>0, 1, 2, 3, 4, 5</pre>
            <p>The output after one shift will be</p>
            <pre>5, 0, 1, 2, 3, 4.</pre>
            <p>Importantly, values of the array are one byte large and the size of the array is such that we never end
                up with accidental out of bounds errors (it has to be divisible by a high enough power of two).</p>
            <details>
                <summary>Click to see the benchmarking code.</summary>
                <p>We define an array <code class="language-x86asm">array</code> with 500 million elements, each of size
                    one byte. We initialise it with some garbage values. We then call the function <code
                        class="language-x86asm">shift_array</code> ten times.</p>
                <pre><code class="language-x86asm">; code that shifts an array 1 byte to the right
; This is cyclical, ie. the last byte becomes the first
GLOBAL main
%define array_len 500_000_000

SECTION .bss
    array resd array_len

SECTION .text

set_array:
    xor eax, eax ; array index

    startsetloop:
        cmp eax, array_len ; if eax >= array_len, go to endsetloop
        jge endsetloop
        mov [array + 4*eax], eax ; array[eax] = eax
        inc eax ; eax++
        jmp startsetloop
    endsetloop:

    ret

shift_array:
    ; Procedure that shifts the array
    ret

main:
    call set_array
    xor eax, eax ; eax = 0

    startmainloop: ; call shift_array 10 times
        cmp rax, 10
        jge endmainloop
        push rax
        call shift_array
        pop rax
        inc rax
        jmp startmainloop
    endmainloop:
    
    xor eax, eax
    ret

</code></pre>
            </details>
            <p>The empty runtime on my computer (an i5-13400F), without any shifts (just starting up the program and
                initialising the array) is 330ms. Anything over that is the runtime taken by our shift code.</p>
            <h4>Naive approach</h4>
            <p>We first try to solve the problem naively. We have two temporary registers, <code
                    class="language-x86asm">al</code> and <code class="language-x86asm">bl</code>. We iterate over the
                entire array repeating the following: store the value of the current element in <code
                    class="language-x86asm">bl</code>, overwrite the element with <code
                    class="language-x86asm">al</code> (representing the previous element) and lastly assign <code
                    class="language-x86asm">al</code> to <code class="language-x86asm">bl</code>. We are left with the
                following code:</p>
            <pre><code class="language-x86asm">shift_array:
    xor eax, eax ; previous value carrier
    xor ecx, ecx ; array index

    startshiftloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endshiftloop
        jge endshiftloop
        mov bl, [array + rcx]  ; \
        mov [array + rcx], al  ;  - swap(al, array[rcx])
        mov al, bl             ; /
        inc rcx ; rcx++
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret</code>
            <p>The code takes 1.595s on average, so that will be our baseline.</p>
            <details>
                <summary>Tangent 1: swapping natively is slow.</summary>
                In x86 there is actually an instruction that swaps two elements. We did not have to use a second intermediate value for it. The code above could be rewritten as:
                <pre><code class="language-x86asm">shift_array:
    xor eax, eax ; previous value carrier
    xor ecx, ecx ; array index

    startshiftloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endshiftloop
        jge endshiftloop
        xchg al, [array + rcx] ; swap(al, array[rcx])
        inc rcx ; rcx++
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret</code>
                <p>This code ends up being over ten times <emph>slower</emph> than our naive approach. This is mostly to do with <code class="language-x86asm">xchg</code> being an atomic operation (which we might look at in the last chapter of the blog series) and probably a bit due to operation dependency (which we looked at last time). It's not important for this post, but it did surprise me just how much slower it is: the runtime using the above code takes 19.136s!</p> 
            </details>
            <details>
                <summary>Tangent 2: A clever approach</summary>
                <p>There is another approach to the program. This time, we are iterating <emph>backwards</emph>. We only have to move each element one byte forward, directly overwriting any elements at that position. This way we don't need a temporary variable for swapping. We still, however, require a variable for moving memory to memory, as x86 does not support memory to memory moves - one operand must be a register.</p>
                <pre><code class="language-x86asm">shift_array:
    mov rcx, array_len ; array index
    dec rcx
    mov al, [array + rcx] ; Store the last value for the cyclical shift

    startshiftloop:
        cmp rcx, 0 ; if rcx <= 0, go to endshiftloop
        je endshiftloop
        ; mov byte [array + rcx], byte [array + rcx - 1] 
        ; we can't do direct memory to memory, so intermediate bl is needed
        mov bl, byte [array + rcx - 1]
        mov byte [array + rcx], bl
        dec rcx ; rcx--
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret</code>
            <p>This code turns out to be very slightly faster, clocking out at an average of 1.435s</p>
            </details>
            <h4>Vectorisation</h4>
            <p>Now we've arrived at the crux of the post: vectorisation. Rather than just moving one element at a time, we can move several elements at once. We've always been accessing one value at a time. But our processor has 64 bit registers, meaning we could read and write 8 elements at once! To move the bytes forward, we simply bit shift them for 8 bits - since this cuts off one byte, we have to store it in a temporary register before the bit shift (and add it back in the next iteration). Since we only have to do one memory read / write per 8 bytes, the resulting code ends up much faster.</p>
            <pre><code class="language-x86asm">shift_array:
    xor eax, eax ; previous value carry
    xor ecx, ecx ; array index

    startshiftloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endshiftloop
        jge endshiftloop
        mov rbx, [array + rcx] ; we read 8 bytes (rbx instead of bl)
        mov rdx, rbx 

        shl rbx, 8 ; shift the values one byte
        or rbx, rax ; add left most bits from the previous iteration

        shr rdx, 56  ; get the left most bits
        mov rax, rdx ; store them into rax

        mov [array + rcx], rbx

        add rcx, 8 ; rcx += 8, we are moving 8 bytes forward
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret </code>
            <p>This code has an average runtime of 0.701s. Accounting for runtime of the initialisation, this is roughly a 4x speedup! We achieved that by using a single instruction (64-bit read and 64-bit bit shift) on multiple data (8 bytes, each one element of the array): SIMD.</p>

            <details>
                <summary>Tangent 3: A clever approach, part 2</summary>
                <pre><code class="language-x86asm">shift_array:
    mov rcx, array_len ; array index
    dec rcx
    mov al, [array + rcx] ; Store the last value for the cyclical shift
    sub rcx, 7

    startshiftloop:
        cmp rcx, 0 ; if rcx <= 0, go to endshiftloop
        je endshiftloop
        mov rbx, [array + rcx - 1]
        mov [array + rcx], rbx ; move the 8 bytes one byte forward
        sub rcx, 8 ; rcx -= 8
        jmp startshiftloop
    endshiftloop:

    mov [array], al
    ret</code>
                <p>If we attempt to use the same clever approach with the SIMD method, our program runtime ends up at 0.675s.</p>
            </details>

            <h4>Other examples</h4>
            <p>Similar approaches can also be used elsewhere. For example, if we want to increment each element of the array we can either iterate byte by byte and increment each one or we could iterate by 8 bytes and add <code class="language-x86asm">0x01010101_01010101</code> to the entire pack of 8. Of course, this only works if there are no overflows (but given that signed overflow is undefined behaviour in many languages, C included, this isn't too outlandish). The overflow restriction, however, stops us from solving the previous blog's topic, the sum of an array, with this approach (at least in an efficient way). We could still attempt to reduce the number of memory accesses by reading 64 bits from memory, rather than 32 bits, but we can only add 32 bits at once in one register (and due to caching, this will not improve the results meaningfully, by my measurements it comes at around 1% improvement over the naive option).</p>
            <p>Another trivial example are the binary operations - these are already vectorised by design, bitwise and applies the <code class="language-x86asm">and</code> operation to every bit in the register.</p>
            <p>An interesting example can be found using the multiplication operation. If we multiply a byte with the number <code class="language-x86asm">0x0101</code>, we duplicate that byte.</p>
            <p>There are many other examples of such operations (I recommend the book Hacker's Delight for further reading), but we move onto the part of the architecture that is responsible for SIMD by design.</p>
            <h3>Vector registers</h3>
            <p>Eventually, the CPU designers realised such operations are useful and extended the x86 architecture with new registers and operations designed for SIMD. The first such extension was <a href="https://en.wikipedia.org/wiki/MMX_(instruction_set)">MMX</a>. These days it's hard to find a processor without MMX support as all mainstream x86-64 Intel and AMD processors support it. MMX comes with additional 64-bit registers. The catch, however, is that instructions don't apply to the entire register, but only part of it. Performing addition on such a register doesn't add two 64-bit numbers together, instead it might compute the sum of the first 32 bits and the last 32 bits separately.</p>
            <h4>MMX</h4>
            <p>Instructions offered in the MMX set are pretty basic. An MMX processor has 8 64-bit registers. There are instructions operating on these registers, as for the standard x86 set. The instructions operate on multiple data stored inside the register; when we do addition of two MMX registers, it adds the constituents independently (as if we performed two independent additions using two independent pairs of 32-bit general purpose registers. Or four pairs of 16-bit or eight pairs of 8-bit registers). Multiple values are "packed" inside the register. There are the following instructions available in the base MMX set
                <ul>
                    <li>Move data in and out of registers</li>
                    <li>Pack data together (e.g. if we have two registers with 2 32-bit integers each, we combine them into one register with 4 16-bit integers)</li>
                    <li>Apply arithmetic operations with overflow</li>
                    <li>Apply arithmetic operations with saturation</li>
                    <li>Apply logical operations</li>
                    <li>Apply bitwise operations and bit shifts</li>
                </ul>
            </p>

            <p>None of these instructions really help us with the array shift, so we return back to the sum of array from the previous blog post. Our goal is to compute the sum of the array and store it in a 32bit value. We don't mind overflows, we are only computing the sum modulo \(2^32\).</p>

            <details>
                <summary>Benchmark code</summary>
                <p>The code is virtually identical to the one for bit shifting, as well as identical to the one from the previous blog post. I am sharing it regardless for completeness.</p>
<pre><code class="language-x86asm">GLOBAL main
%define array_len 500_000_000

SECTION .bss
    align 16
    array resd array_len

SECTION .text

set_array:
    xor eax, eax ; array index

    startsetloop:
        cmp eax, array_len ; if eax >= array_len, go to endsetloop
        jge endsetloop
        mov [array + 4*eax], eax ; array[eax] = eax
        inc eax ; eax++
        jmp startsetloop
    endsetloop:

    ret

sum_array:
    ; Procedure that sums the array
    ret

main:
    call set_array
    xor eax, eax ; eax = 0

    startmainloop: ; call sum_array 10 times
        cmp rax, 10
        jge endmainloop
        push rax
        call sum_array
        pop rax
        inc rax
        jmp startmainloop
    endmainloop:
    
    xor eax, eax
    ret</code>
            </details>

            <p>The initialisation of the array takes roughly 300ms as before. The naive approach takes 1.669s on average</p>
            <pre><code class="language-x86asm">sum_array:
    xor eax, eax ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        add eax, [array + 4*rcx] ; eax += array[rcx]
        inc rcx ; rcx++
        jmp startsumloop
    endsumloop:


    mov rdi, helloMsg
    mov esi, eax
    xor eax, eax
    call printf

    ret
</code></pre>

            <p>This time we solve it using MMX registers. We first write a toy program to introduce the registers and
                operations, then quickly write the code for summing the array.</p>

            <p>First, we have to mention that the MMX registers are actually shared with the floating point registers.
                This has no impact on us, as we are not using any floating point numbers, but do not mix MMX code and
                x87 floating point code. That's also the reason we have to use the instruction <code
                    class="language-x86asm">emms</code> after finishing with MMX instructions, it exits the MMX mode and
                clears the slate. We address the registers using <code class="language-x86asm">mm0, ...m mm7</code>.
                Unlike when using floating point operations, here they are addressible without the use of a stack. They
                also have their own operations. For instance, instead of <code class="language-x86asm">mov</code> we
                have to use <code class="language-x86asm">movq</code>, instead of <code
                    class="language-x86asm">add</code> we use <code class="language-x86asm">paddd</code> and so on.
                Let's look at a code sample.</p>
            <pre><code class="language-x86asm">GLOBAL main
EXTERN printf

SECTION .data
    value dq 0x0000000400000002  ; 64-bit value (e.g., two 32-bit values)
    printmsg: db "%d", 0
SECTION .text

main:
    mov eax, 1
    mov ebx, 6
    movd mm0, eax ; mm0 has 0x1
    movd mm1, ebx ; mm1 has 0x6
    
    xor eax, eax ; clear data. Not needed
    xor ebx, ebx ; Just to demonstrate we are using mmx only

    packssdw mm0, mm1 ; (1) mm0 has 0x0000000100000006

    movq mm1, [value] ; mm1 has 0x0000000400000002

    paddd mm0, mm1 ; (2) mm0 has 0x0000000500000008

    movd eax, mm0 ; eax has 0x8
    psrlq mm0, 32 ; (3) bitshift the high 32 bits into the low 32 bits
    movd ebx, mm0 ; ebx has 0x5

    emms
    
    add eax, ebx ; eax has 0xC

    mov rdi, printmsg
    mov esi, eax
    xor eax, eax
    call printf

    xor eax, eax
    ret</code></pre>

            <p>Some parts of the code are pretty self explanatory. We initialise <code
                    class="language-x86asm">[value]</code> with a 64-bit value that we see as two 32-bit values. We can
                move data into the lowest part of a MMX register using <code class="language-x86asm">movd</code> (d
                stands for doubleword, 32 bits; q would stand for quadword or 64 bits). Then at (1) we pack two MMX
                registers into one. The mnemonic <code class="language-x86asm">packssdw</code> stands for <bold>pack
                </bold>, using <bold>S</bold>igned <bold>S</bold>aturation, 4 <bold>D</bold>oublewords (two from MM0,
                two from MM1) into 4 <bold>W</bold>ords. So it casts both 32 bit components of MMX registers into two 16
                bit components (and, if they are too large to fit, saturates them by setting them to the highest 16 bit
                value), then concatenates them to the (also converted and saturated) 16 bit components of the other MMX
                register.</p>

            <p>With <code class="language-x86asm">paddd</code> at (2) we add both doubleword components independently.
                We compute the sum of the first 32 bits of both registers and store it into the first 32 bits of mm0;
                then we compute the sum of the last 32 bits of both registers and store it into the second half of mm0.
                If we wanted to add 8 separate byte-large components, we would use <code
                    class="language-x86asm">paddb</code>. Lastly, we apply bitshift to the entire quadword MMX register,
                to move the highest 32 bits (representing the other integer) into the low 32 bits, that are easily
                extracted into the general purpose registers. The code outputs 13, as desired.</p>

            <p>We now write the code leveraging MMX for the sum of an array.</p>
            <pre><code class="language-x86asm">sum_array:
    pxor mm0, mm0 ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        paddd mm0, [array + 4*rcx] ; Compute sum of two elements of the array
        add rcx, 2 ; rcx+=2, we are processing two elements at a time
        jmp startsumloop
    endsumloop:

    movd eax, mm0
    psrlq mm0, 32
    movd ecx, mm0
    add eax, ecx

    emms

    ret</code></pre>
            <p>Compared to the naive code, this program computes two sums at once. The average runtime now is
                approximately 1.3306s. Interestingly (and somewhat expectedly), this is roughly as fast as our final
                program in the previous post, where we computed two sums in parallel (just like here), except we used
                instruction-level parallelism instead of SIMD parallelism. Now in the words of Goku, "And this is to go
                even further beyond!"; we explore other extensions of x86 which offer even higher levels of SIMD
                parallelism, as well as more flexibility. MMX itself is quite old and some of the design choices are
                questionable.</p>

            <h4>SSE</h4>
            <p><a href="https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions">SSE</a> stands for Streaming SIMD
                Extensions. It was originally introduced in 1999, but it went through a few revisions, we're currently
                at SSE4.2. This extension adds 8 new XMM registers (16 in x86-64. Do not confuse XMM with MMX).
                Crucially, these registers are no longer 64 bit, they are 128 bit large (and extended further with the
                AVX extension, which we will look at at the end). The original SSE allowed SIMD operations of four
                32-bit floats. SSE2 extended the support to 2 64-bit floats, as well as all kinds of integer SIMD
                instructions (for example operations on four 32-bit integers packed into a 128-bit register). SSE3 added
                some instructions that are not just the default SIMD, but include horizontal operations. For example,
                <code class="language-x86asm">haddpd</code> takes two 128-bit XMM registers, the first one storing
                64-bit floats \(a_1\) and \(a_2) and the second one storing 64-bit floats \(b_1\) and \(b_2\). The
                result of the operation is an XMM register storing two 64-bit floats \(a_1+a_2\) and \(b_1+b_2\)
                (whereas vertical addition would carry \(a_1+b_1\) and \(a_2+b_2\)). SSE4 includes a variety of new
                instructions. I will not touch upon all of them as they can be a bit niche, but the most imporant ones
                (in my opinion) are <code class="language-x86asm">DPPS</code> for computing a dot product and <code
                    class="language-x86asm">pminsd</code>, which computes a packed minimum. You can check the full
                instruction set here: <a
                    href="https://www.intel.com/content/dam/develop/external/us/en/documents/d9156103-705230.pdf">https://www.intel.com/content/dam/develop/external/us/en/documents/d9156103-705230.pdf</a>.
            </p>

            <p>Now it's time to write a four-way parallel version of the sum program, this time using the 128-bit SSE
                instructions, rather than the 64-bit MMX ones. We are using three operations. One, <code
                    class="language-x86asm">paddd</code> is simple, it just adds four 32-bit integers at once. The
                instruction <code class="language-x86asm">movhlps</code> is a bit more complex. It takes a register
                containing four integers</p>
            <pre>xmm0 = [a, b, c, d]</pre>
            <p> and moves the values from the high 64 bits to the low 64 bits. The result of the operation <code
                    class="language-x86asm">movhlps xmm1, xmm0</code> is </p>
            <pre>xmm1 = [c, d, ?, ?]</pre>
            <p>Lastly, we need <code class="language-x86asm">pshufd</code>. This allows us to shuffle the integers in an
                XMM register arbitrarily. It takes three operands, the result of the suffle, the source of the shuffle
                and an index that describes which element goes where. The index is an 8-bit integer, so it can be
                partitioned into four 2-bit chunks. Each chunk tells you which integer from the source register it gets.
                For example, if we have</p>
            <pre>xmm0 = [a, b, c, d]</pre>
            <p>And we take the index value 100, which is 0b01 10 01 00 in binary. We convert each chunk back to decimal,
                to get 1, 2, 1, and 0. Thus the destination gets the 2nd element of source in the first element of the
                destination (2nd element due to zero indexing), 3rd element of source in the second element of the
                destination and so on. So the output of <code class="language-x86asm">pshufd xmm1, xmm0, 100</code> is
            </p>
            <pre>xmm1 = [b, c, b, a]</pre>
            <p>Last word of warning, before we write the code: the data we're operating on has to be 16-byte aligned. In
                our benchmark code, this is achieved using <code class="language-x86asm">align 16</code> before the
                definition of the array. Now it's time to write our code.</p>
        </article>

        <pre><code class="language-x86asm">sum_array:
    pxor xmm0, xmm0 ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        paddd xmm0, [array + 4*rcx]
        add rcx, 4 ; rcx+=4
        jmp startsumloop
    endsumloop:

    ; xmm0 has 32-bit integers [a, b, c, d] 
    movhlps xmm1, xmm0 ; Move high bits from xmm0 -> low bits of xmm1
    ; xmm1 has [c, d, ?, ?]
    paddd xmm0, xmm1
    ; xmm0 has [a+c, b+d, ?, ?]
    pshufd xmm1, xmm0, 0x1
    ; xmm1 has [b+d, a+c, a+c, a+c]
    paddd xmm0, xmm1
    ; xmm0 has [a+c + b+d, b+d + a+c, ?, ?]
    movd eax, xmm0
    ; eax has a+c + b+d = a+b+c+d = sum of the array

    ret
</code></pre>
        <p>As expected due to the further parallelisation, this is another speedup and the runtime is now approximately
            1.181s. Let's continue with the next extension.</p>

        <h4>AVX</h4>
        <p><a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a> stands for Advanced Vector
            Extensions. This one, too, comes in several versions: AVX, AVX2 and AVX-512 (and technically AVX10, but this
            one doesn't really add anything). The extension further extends the SSE registers to 256 bits or, in the
            case of AVX-512, to 512 bits (or 64 bytes, capable storing 16 32-bit integers). It also comes with a more
            complex coding scheme, <a href="https://en.wikipedia.org/wiki/VEX_prefix">VEX coding</a>. Previously, all
            instructions only worked on up to two operands (and optionally a fixed constant, an immediate, in addition,
            as we've seen in <code class="language-x86asm">pshufd</code>) while the new scheme allows up to four
            operands (and an additional immediate). Most of the instructions are just adaptations of the SSE
            instructions to the larger registers and the VEX coding scheme, but there's also a variety of new
            instructions, mostly to do with shuffling and rearranging the data in registers.</p>
        <p>If SSE registers are addressed using <code class="language-x86asm">xmm0, ..., xmm15</code>, the 256-bit AVX
            registers are addressed using <code class="language-x86asm">ymm0, ..., ymm15</code> and the 512-bit ones
            using <code class="language-x86asm">zmm0, ..., zmm15</code>. If our CPU supports AVX-512, we have 32
            registers, so they all go up to <code class="language-x86asm">xmm31 / ymm31 / zmm31</code>. While every x86
            computer supports MMX and every modern computer supports SSE, AVX support is still a bit rare and AVX-512
            even moreso. I actually don't have access to AVX-512 at home, so I have to resort to using the university
            HPC.</p>


        <details>
            <summary>Tangent 4: VEX coding on non-vector registers</summary>
            While this was not the case originally, they have extended the use of VEX coding to general purpose
            registers, such as <code class="language-x86asm">eax</code>. They are mostly limited to the bit manipulation
            instructions, however. Let's check one example.
            <pre><code class="language-x86asm">mov eax, 0x0000_0F0F
mov ebx, 0x0000_F00F
mov ecx, ebx ; First copy
not ecx ; ~ebx = 0xFFFF_0FF0
and ecx, eax ; ecx = ~ebx & eax = 0x0000_0F00 = 3840
            </code></pre>
            <p>With VEX coding, this can be rewritten easily as:
            <pre><code class="language-x86asm">mov eax, 0x0000_0F0F
mov ebx, 0x0000_F00F
andn ecx, ebx, eax
</code></pre>
        </details>

        <details>
            <summary>Tangent 5: false AVX dependencies</summary>
            <p>AVX extensions made an interesting choice with their use of the SSE registers. See, when we use 32-bit
                instructions on 64-bit general purpose registers, it tends to zero-out the higher 64 bits regardless;
                this is why when we want to zero out a register, we use <code
                    class="language-x86asm">xor eax, eax</code> instead of <code
                    class="language-x86asm">xor rax, rax</code>. It's slightly more efficient (it's one byte shorter in
                machine code) and it the output is the same in both cases, every bit of the 64-bit register <code
                    class="language-x86asm">rax</code> becomes zero. However when we use the SSE instructions (without
                their VEX-coded form) <emph>that doesn't happen</emph>. Only the lower 128 bits are modified. This
                doesn't seem like a big deal, but it can create a false dependency and make the optimisation much harder
                for the processor (exactly the kind of optimisation we relied on in the previous blog post, in fact).
                They added an instruction <code class="language-x86asm">vzeroupper</code> for this (which zeroes out the
                upper bits, eliminating the stalls), but this still adds overhead. We can bypass that by not mixing SSE
                and AVX or, if we do have to mix them, use the VEX coded form of SSE operations, which does zero out the
                higher bits.</p>
            <p>A similar thing sometimes happens with the use of <code class="language-x86asm">inc</code> (increment)
                instruction. While the <code class="language-x86asm">add eax, 1</code> does the full update of the
                condition flags, <code class="language-x86asm">inc eax</code> only does a partial update which can cause
                some funny stalls due to false dependencies and slow the code down. This has mostly been mitigated after
                Pentium 4 and there's usually no performance difference between the two instructions these days.</p>
        </details>

        <p>On with the code now. This time we extract the data from the AVX registers a bit differently than before. A
            similar method as before would still work, but as I am using this blog post to give an introduction to the
            new SIMD instructions, I believe it would be beneficial to introduce some more instructions. First one is
            <code class="language-x86asm">vextractf128</code>. This one can extract 128 bits from a 256-bit register and
            store it into an 128-bit register. The second instruction is <code class="language-x86asm">phaddd</code>,
            which performs horizontal addition. So given two registers <code
                class="language-x86asm">xmm0 ; = [a, b, c, d]</code> and <code
                class="language-x86asm">xmm1 ; = [e, f, g, h]</code>, the result of <code
                class="language-x86asm">phaddd xmm0, xmm1</code> is <code
                class="language-x86asm">xmm0 ; = [a+b, c+d, e+f, g+h]</code>. We can use this to coalesce the results
            from 8 independent integers into one sum.</p>
        <p>We also have to make sure that our memory is aligned to 32-byte boundaries.</p>
        <pre><code class="language-x86asm">SECTION .bss
    align 32 ; 64 for AVX-512
    array resd array_len</code></pre>
        <p>We are left with the following code for the sum using AVX.</p>
        <pre><code class="language-x86asm">sum_array:
    vpxor ymm0, ymm0, ymm0 ; partial sum carrier
    xor ecx, ecx ; array index

    startsumloop:
        cmp rcx, array_len ; if rcx >= array_len, go to endsumloop
        jge endsumloop
        vpaddd ymm0, ymm0, [array + 4*rcx]
        add rcx, 8 ; rcx+=8
        jmp startsumloop
    endsumloop:

    ; ymm0 = [a0, a1, a2, a3, a4, a5, a6, a7]
    vextractf128 xmm1, ymm0, 1  ; xmm1 = [a4, a5, a6, a7]
    paddd xmm0, xmm1      ; xmm0 = [a0+a4, a1+a5, a2+a6, a3+a7]
    phaddd xmm0, xmm0      ; xmm0 = [ (a0+a4)+(a1+a5), (a2+a6)+(a3+a7) ]
    phaddd xmm0, xmm0      ; xmm0[0] = (a0+a4)+(a1+a5)+(a2+a6)+(a3+a7)

    movd eax, xmm0
    ret</code></pre>
        <p>And the speed of the code is now 1.074s. We are starting to see some diminishing returns, but we observe a
            speedup regardless. Lastly, the code using AVX-512. This is more for completeness, I will not dwell on it
            for too long due to the lack of availability of the AVX-512 instruction set. The code is essentially the
            same as the 256-bit one though.</p>
        <pre><code class="language-x86asm">sum_array:
    vpxord zmm0, zmm0, zmm0 ; partial sum carrier
    xor ecx, ecx ; array index
    
    startsumloop:
        cmp rcx, array_len            ; if index >= array_len, go to endsumloop
        jge endsumloop
        vpaddd zmm0, zmm0, [array + 4*rcx]
        add rcx, 16   
        jmp startsumloop
    
    endsumloop:
        ; zmm0 contains 16 partial sums.
        vextracti32x8 ymm1, zmm0, 1 ; Extract upper 256 bits into ymm1
        vpaddd ymm0, ymm0, ymm1 ; Add lower and upper 256-bit halves => ymm0 now has 8 ints
        vextracti32x4 xmm1, ymm0, 1 ; Extract upper 128 bits into xmm1
        vpaddd xmm0, xmm0, xmm1 ; Add lower and upper 128-bit halves => xmm0 now has 4 ints
        phaddd xmm0, xmm0, xmm0 ; Horizontal add: now xmm0 contains 2 partial sums in its lower half
        phaddd xmm0, xmm0, xmm0 ; Final horizontal add: xmm0[0] holds the total sum
    
        movd eax, xmm0
        ret
    </code></pre>
        <h4>Summary of results</h4>
        <p>Now let's bring all results together. I am also running the code on another computer, the HPC with a Xeon
            Silver 4410Y. This way, we can also measure the impact of AVX-512, but the performance will not be good as
            it's also running other things (it is, however, consistent).</p>
        <table>
            <th>
            <td></td>
            <td>i5-13400F</td>
            <td>Xeon 4410Y</td>
            </th>
            <tr>
                <td>Naive</td>
                <td>1.669s</td>
                <td>3.400s</td>
            </tr>
            <tr>
                <td>MMX</td>
                <td>1.330s</td>
                <td>3.076s</td>
            </tr>
            <tr>
                <td>SSE</td>
                <td>1.181s</td>
                <td>2.717s</td>
            </tr>
            <tr>
                <td>AVX</td>
                <td>1.074s</td>
                <td>2.364s</td>
            </tr>
            <tr>
                <td>AVX-512</td>
                <td>N/A</td>
                <td>2.031s</td>
            </tr>
        </table>

        <p>The results we obtained are quite satisfactionary. We managed to obtain a massive speedup by using the SIMD
            instructions and, hopefully, learned something along the way. As a curiosity, if we compile the
            naively-written C code using the highest optimisation levels and <code>-march=native</code>, it compiles it
            to a version roughly equivalent to our AVX code (with roughly the same performance). It's still good to be
            aware of the SIMD instructions, as sometimes the compiler won't recognise the optimisations (or only
            recognises them if we write our C code in the correct way).</p>

        <p>If you have any further questions or comment, drop me a mail! I would be happy to discuss anything further.
        </p>


    </main>
    <!-- mention GPUs -->
    <footer>
        <p>© 2025 Tadej Petrič</p>
    </footer>
</body>

</html>